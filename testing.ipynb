{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhdbscan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from hdbscan import HDBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import BertTokenizer as bt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if needed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Ensure necessary NLTK downloads\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load reviews from a CSV file\n",
    "df1 = pd.read_csv('sentiment_reviews_withcount.csv')\n",
    "reviews = df1['review_text'].tolist()  # Replace 'review_text' with your column name if different\n",
    "\n",
    "\n",
    "# Preprocess reviews: Tokenization, removing stopwords, non-alphabetical characters\n",
    "def preprocess_text(texts):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_texts = [\n",
    "        [word for word in word_tokenize(document.lower()) if word.isalpha() and word not in stop_words]\n",
    "        for document in texts]\n",
    "    return preprocessed_texts\n",
    "\n",
    "\n",
    "preprocessed_reviews = preprocess_text(reviews)\n",
    "\n",
    "# Create a dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(preprocessed_reviews)\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_reviews]\n",
    "\n",
    "# Apply LDA\n",
    "num_topics = 5  # Adjust based on your data and needs\n",
    "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=15,\n",
    "                            iterations=100)\n",
    "\n",
    "# Additional stopwords for category refinement\n",
    "additional_stopwords = {'get', 'great', 'like', 'really', 'good', 'gym', 'place', 'love', 'hate', 'one', 'trainer'}  # Add more words as needed\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # Stemming (uncomment if desired)\n",
    "    # words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def get_bert_embeddings(sentences):\n",
    "    # Tokenize sentences\n",
    "    tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**tokenized_sentences)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def calculate_center(df):\n",
    "    centers = {}\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_embeddings = df[df['Cluster'] == cluster]['bert_embeddings']\n",
    "        center = cluster_embeddings.apply(pd.Series)\n",
    "        center = center.mean()\n",
    "        centers[cluster] = center.tolist()\n",
    "    return centers\n",
    "\n",
    "\n",
    "def find_closest_sentence(df, centers):\n",
    "    closest_sentences = {}\n",
    "    for cluster, center in centers.items():\n",
    "        distances = [np.linalg.norm(np.pad(embedding, (0, len(center) - len(embedding)), 'constant') - center)\n",
    "                        for\n",
    "                        embedding in df[df['Cluster'] == cluster]['Vector'].values]\n",
    "        closest_index = distances.index(min(distances))\n",
    "        closest_sentences[cluster] = df[df['Cluster'] == cluster]['Sentences'].values[closest_index]\n",
    "    return closest_sentences\n",
    "\n",
    "\n",
    "def get_combined_categories(ldamodel, num_topics, num_keywords=5):\n",
    "    # Collect all words from all topics\n",
    "    all_keywords = []\n",
    "    for i in range(num_topics):\n",
    "        topic_terms = ldamodel.show_topic(i)\n",
    "        all_keywords.extend([word for word, _ in topic_terms])\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    keyword_counts = Counter(all_keywords)\n",
    "\n",
    "    # Filter out additional stopwords\n",
    "    filtered_keywords = {word: count for word, count in keyword_counts.items() if word not in additional_stopwords}\n",
    "\n",
    "    # Get the most common words across all topics, after filtering\n",
    "    most_common_keywords = [word for word, count in Counter(filtered_keywords).most_common(num_keywords)]\n",
    "    return most_common_keywords\n",
    "\n",
    "\n",
    "#review data type -- sentiment empty\n",
    "# -->\n",
    "\n",
    "# Sample gym reviews\n",
    "#reviews = [\n",
    "    #\"The staff at this gym are incredibly friendly and helpful. They always go the extra mile to make sure I have a great workout experience.\",\n",
    "    #\"The equipment is top-notch and well-maintained. They have a wide variety of machines for all my training needs.\",\n",
    "   # \"The gym is always clean and well-organized. It's a pleasure to work out in such a pleasant environment.\",\n",
    "  #  \"The staff could be a bit more attentive, but the equipment is good overall.\",\n",
    " #   \"This gym is a bit dirty at times, but the staff is friendly and the classes are great.\",\n",
    "#]\n",
    "\n",
    "df = pd.DataFrame(reviews, columns=['Sentences'])\n",
    "\n",
    "# Define your labels\n",
    "labels = get_combined_categories(lda_model, num_topics)\n",
    "\n",
    "# Preprocess reviews\n",
    "processed_reviews = [preprocess_text(review) for review in reviews]\n",
    "df['processed_sentences'] = processed_reviews\n",
    "\n",
    "\n",
    "label_tracker_dict = {}\n",
    "\n",
    "for i in range(0,len(labels)):\n",
    "    label_tracker_dict[i] = labels[i]\n",
    "\n",
    "# Convert labels to text for feature representation\n",
    "label_texts = [\" \".join([label, \"review is\"]) for label in labels]\n",
    "\n",
    "# Feature engineering with TF-IDF\n",
    "\n",
    "tokenizer = bt.from_pretrained('bert-base-uncased')\n",
    "# Get BERT embeddings for sentences\n",
    "embeddings = get_bert_embeddings(processed_reviews)\n",
    "label_embeddings = get_bert_embeddings(label_texts)\n",
    "# Save embeddings to DataFrame\n",
    "df['bert_embeddings'] = embeddings.tolist()\n",
    "\n",
    "\n",
    "# HDBSCAN clustering\n",
    "clusterer = HDBSCAN(min_cluster_size=2,  # Allow any cluster size\n",
    "                             min_samples=2,         # Ensure exactly three clusters\n",
    "                             metric='euclidean',\n",
    "                             cluster_selection_method='leaf', # Choose 'eom' to automatically select the number of clusters\n",
    "                             prediction_data=True)\n",
    "  # Adjust parameters as needed\n",
    "clusterer.fit(embeddings)\n",
    "\n",
    "# Cluster centroids\n",
    "cluster_labels = clusterer.labels_\n",
    "\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "centers = calculate_center(df)\n",
    "\n",
    "# Cosine similarity with threshold -- fix cluster centroid here\n",
    "threshold = 0.62 # Adjust as needed\n",
    "assigned_labels = {}\n",
    "for cluster_id in centers.keys():\n",
    "    centroid = centers[cluster_id]\n",
    "    if cluster_id == -1:\n",
    "        assigned_labels[cluster_id] = [-1]\n",
    "        continue\n",
    "    centroid = pd.Series(centroid)\n",
    "    assigned_labels[cluster_id] = []\n",
    "    for label_id, label_vector in enumerate(label_embeddings):\n",
    "        label_vector = pd.Series(label_vector)\n",
    "        similarity = cosine_similarity(centroid.values.reshape(1, -1), label_vector.values.reshape(1, -1))[0][0]\n",
    "        if similarity > threshold:\n",
    "            curr_labels = assigned_labels[cluster_id]\n",
    "            curr_labels.append(label_id)\n",
    "            assigned_labels[cluster_id] = curr_labels\n",
    "\n",
    "for cluster_id in centers.keys():\n",
    "    if (len(assigned_labels[cluster_id]) == 0):\n",
    "        centroid = centers[cluster_id]\n",
    "        if (assigned_labels[cluster_id] == [-1]):\n",
    "            continue\n",
    "        centroid = pd.Series(centroid)\n",
    "        maximum = -1\n",
    "        for label_id, label_vector in enumerate(label_embeddings):\n",
    "            label_vector = pd.Series(label_vector)\n",
    "            similarity = cosine_similarity(centroid.values.reshape(1, -1), label_vector.values.reshape(1, -1))[0][0]\n",
    "            if similarity > maximum:\n",
    "                assigned_labels[cluster_id] = [label_id]\n",
    "                maximum = similarity\n",
    "\n",
    "# assign labels to all sentences seperately who are in cluster -1:\n",
    "\n",
    "for i in range(0, len(processed_reviews)):\n",
    "    if df['Cluster'][i] == -1:\n",
    "        maximum = -1\n",
    "        for label_id, label_vector in enumerate(label_embeddings):\n",
    "            label_vector = pd.Series(label_vector)\n",
    "            similarity = cosine_similarity(embeddings[i].reshape(1, -1), label_vector.values.reshape(1, -1))[0][0]\n",
    "            if similarity > maximum:\n",
    "                df['Cluster'][i] = label_id\n",
    "                maximum = similarity\n",
    "\n",
    "\n",
    "\n",
    "df['assigned_label'] = df['Cluster'].map(assigned_labels)\n",
    "df['named_labels']  = df['assigned_label'].apply(lambda x: [label_tracker_dict[num] for num in x])\n",
    "print('breakpoint')\n",
    "\n",
    "for i in range(0, len(processed_reviews)):\n",
    "    for j in labels:\n",
    "        if j in processed_reviews[i]:\n",
    "            if j in df['named_labels'][i]:\n",
    "                continue\n",
    "            else:\n",
    "                curr = df['named_labels'][i]\n",
    "                curr.append(j)\n",
    "# Print results\n",
    "print(\"Reviews:\")\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"- Review {i+1}: {review}\")\n",
    "\n",
    "print(\"\\nClusters and assigned labels:\")\n",
    "for cluster_id, labels in assigned_labels.items():\n",
    "    print(f\"- Cluster {cluster_id+1}:\", \", \".join(str(labels)))\n",
    "\n",
    "#import textblob and run it on all the reviews and add a value of sentiment and polarity to the dataframe\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "sentiments = []\n",
    "polarities = []\n",
    "#use the normal reviews\n",
    "for review in reviews:\n",
    "    blob = TextBlob(review)\n",
    "    sentiments.append(blob.sentiment[0] * 2.5 + 2.5)\n",
    "    polarities.append(blob.sentiment[1]  * 2.5 + 2.5)\n",
    "#now add to dataframe\n",
    "df['sentiment'] = sentiments\n",
    "df['polarity'] = polarities\n",
    "categories_summaries_sentiments = defaultdict(list)\n",
    "categores_summaries_polarities = defaultdict(list)\n",
    "for i in range(0, len(df)):\n",
    "    for j in df['named_labels'][i]:\n",
    "        categories_summaries_sentiments[j].append(df['sentiment'][i])\n",
    "        categores_summaries_polarities[j].append(df['polarity'][i])\n",
    "\n",
    "\n",
    "df_summaries = pd.DataFrame(columns=['Category', 'Average Sentiment', 'Average Polarity'])\n",
    "for i in categories_summaries_sentiments.keys():\n",
    "    #add a row to the dataframe using df.loc\n",
    "    df_summaries.loc[len(df_summaries)] = [i, sum(categories_summaries_sentiments[i])/len(categories_summaries_sentiments[i]), sum(categores_summaries_polarities[i])/len(categores_summaries_polarities[i])]\n",
    "print(df_summaries)\n",
    "\n",
    "print(df)\n",
    "\n",
    "#store df to store the sentences\n",
    "#store df_summaries to store scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
